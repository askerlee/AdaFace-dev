model:
  base_lr: 2e-06
  weight_decay: 0.0

  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    automatic_optimization: True
    # linear_start/linear_end: params for linear beta schedule.
    beta_schedule:    linear
    linear_start:     0.00085
    linear_end:       0.0120
    log_every_t:      200
    timesteps:        1000
    first_stage_key:  image
    cond_stage_key:   caption
    image_size:       64
    channels:         4
    # Since we want to train the embedding manager, we set embedding_manager_trainable = True.
    embedding_manager_trainable:      True
    monitor:                          train/loss
    scale_factor:                     0.18215
    unfreeze_unet:                    False
    unet_lr:                          0.0
    use_fp_trick:                     True
    comp_distill_iter_gap:            3
    comp_distill_denoising_steps_range: [2, 2]
    unet_teacher_types:               ['consistentID', 'arc2face']
    p_unet_teacher_uses_cfg:          0.6
    unet_teacher_cfg_scale_range:     [1.5, 2.5]
    # Reduce p_unet_distill_uses_comp_prompt from 0.2 to 0.1, since the joint teachers
    # only have weak compositionality and their outputs are not so informative.
    p_unet_distill_uses_comp_prompt:  0
    # unet_distill_iter_gap determines the percentage of unet_distill_iter among do_normal_recon iters.
    # unet_distill_iter_gap = 2 means at 50% of the non-compositional iters, 
    # we use the unet teacher's output as the target (do_unet_distill).
    # at the other 50%, we use the original image as the target (do_normal_recon).
    unet_distill_iter_gap:            2
    unet_distill_weight:              8
    # randomly generated face IDs seem not to make sense for joint ID encoders, since the two parts
    # of the IDs (belonging to two face detectors) are not consistent. However, a small proportion
    # of random face IDs may help force the arc2face Ada focuses on reconstructing the arc2face random ID,
    # and the consistentID Ada focuses on reconstructing the consistentID random ID. Therefore, it may still
    # be helpful to set a small p_gen_rand_id_for_id2img. (If both parts are reconstructed well, then their summation
    # should match the mixture of the two teachers' outputs.)
    p_gen_rand_id_for_id2img:         0
    p_perturb_face_id_embs:           0.4
    # ** The perturbation here is not to make the img2ada encoder more robust to random perturbations,
    # ** but to find neighbors of the subject image embeddings for UNet distillation.
    # After perturbation, consistentID embeddings and arc2face embeddings are slightly inconsistent.
    # Therefore, for jointIDs, we should reduce perturb_face_id_embs_std_range to [0.3, 0.6]. 
    # Otherwise, the inconsistency may lead to a noisy divergence between the student and the teacher outputs.
    perturb_face_id_embs_std_range:   [0.1, 0.2]
    # typical delta reg loss: 0.85, * weight -> 1.7e-4
    prompt_emb_delta_reg_weight:      1e-5
    # Set cls_subj_mix_scale to 1 to use only class embeddings.
    # 0.7: 0.7 * class embeddings + 0.3 * subj embeddings.
    cls_subj_mix_scale:               0.7
    comp_fg_bg_preserve_loss_weight:  3e-3
    # DISABLED: subj_bg_suppress loss used in recon iters.
    # loss_recon_subj_bg_suppress: 1.5, * weight -> 1.5e-2
    recon_subj_bg_suppress_loss_weight: 1e-2
    subj_attn_norm_distill_loss_weight: 0
    # Only enabled if loss_sc_mc_bg_match == 0.
    feat_delta_align_loss_weight:      1e-4
    pred_l2_loss_weight:               0 #1e-4

    # AdamW, AdamW8bit, Adam8bit, NAdam, Prodigy
    optimizer_type:           CAdamW 
    # This grad_clip is used when manual optimization is enabled.
    grad_clip:                0.5
    adam_config:
      # Half life:
      # 0.995  0.996  0.997  0.998  0.999
      # 138    172    231    346    693
      # We set beta2 to 0.995 for faster update of the estimated variances.
      betas:                  [0.9,  0.999]  
      scheduler_config:
        target: ldm.modules.lr_scheduler.LambdaWarmUpCosineScheduler
        params:
          verbosity_interval: 0
          warm_up_steps:      500
          lr_start:           0.01
          lr_max:             1.0
          lr_min:             0.1

    prodigy_config:
      betas:    [0.985, 0.993]   # Faster to converge than zs_betas.
      zs_betas: [0.9,   0.999]   # Slower to converge than betas.
      d_coef:           5
      warm_up_steps:    500
      # 1 cycle after the warm_up_steps.
      # Each cycle is linearly decreasing the LR from base_lr to 0.09*base.
      scheduler_cycles: 1
      # CyclicLR, CosineAnnealingWarmRestarts, Linear. 
      # Linear or CosineAnnealingWarmRestarts doesn't work well.
      scheduler_type: 'Linear'
      
    personalization_config:
      target: ldm.modules.embedding_manager.EmbeddingManager
      params:
        subject_strings:                    ['z']
        subj_name_to_cls_delta_string:      { 'z': "person", 'rand_id_to_img_prompt': 'person' }
        training_perturb_std_range:         [ 0.05, 0.1 ]
        # probs for recon_iter and compos_distill_iter, respectively
        training_perturb_prob:              { 'recon_iter':            0.6, 
                                              'unet_distill_iter':     0.3,
                                              'compos_distill_iter':   0.3 }
        adaface_ckpt_paths:                 null
        adaface_encoder_types:              ['consistentID', 'arc2face']
        # Probabilities for randomly dropping out each of the ada encoders.
        p_encoder_dropout:                  0
        # Load the ada components from these placeholders in the checkpoint.
  
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 32 # unused
        in_channels: 4
        out_channels: 4
        model_channels: 320
        attention_resolutions: [ 4, 2, 1 ]
        num_res_blocks: 2
        channel_mult: [ 1, 2, 4, 4 ]
        num_heads: 8
        # The spatial transformer is just a transformer applied on 4D (BCHW) inputs.
        use_spatial_transformer: True
        transformer_depth: 1
        context_dim: 768
        use_checkpoint: True
        legacy: False
        
    first_stage_config:
      target: ldm.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 4
        monitor: train/loss_recon
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 512
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0

        lossconfig:
          target: torch.nn.Identity

    cond_stage_config:
      target: ldm.modules.encoders.modules.FrozenCLIPEmbedder
      params:
        last_layers_skip_weights: [0.5, 0.5]

data:
  target: main.DataModuleFromConfig
  params:
    # Since we optimize two subj basis generators, we can only afford a batch size of 2.
    # Effective batch size is 2 * 2 GPUs * 4 accum_grad_batches = 16.
    batch_size:  3
    num_workers: 4
    wrap: false
    max_steps: 60000
    train:
      target: ldm.data.personalized.PersonalizedBase
      params:
        size: 512       # Image resolution
        set_name: train
        repeats: 100
        verbose: false

lightning:
  modelcheckpoint:
    params:
      every_n_train_steps: 500

  trainer:
    benchmark: True
    max_steps: 60000
    num_sanity_val_steps: 0
    limit_val_batches:    0  # Disable doing validation.
    deterministic: False
    replace_sampler_ddp: False
    accumulate_grad_batches: 2
    # clip gradients' global norm to <=0.5 using gradient_clip_algorithm='norm' by default    
    gradient_clip_val: 0.5
    
